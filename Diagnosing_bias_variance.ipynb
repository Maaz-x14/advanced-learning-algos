{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for building linear regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# import lab utility functions in utils.py\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into train, cv, and test\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data1.csv')\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training inputs (1 feature):\\n {x_train[:5]}\\n\")\n",
    "\n",
    "# Instantiate the regression model class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train and plot polynomial regression models\n",
    "utils.train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and plot polynomial regression models. Bias is defined lower.\n",
    "utils.train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Try getting additional features\n",
    "\n",
    "Another thing you can try is to acquire other features. Let's say that after you got the results above, you decided to launch another data collection campaign that captures another feature. Your dataset will now have 2 columns for the input features as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data2.csv')\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training inputs (2 features):\\n {x_train[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train and plot polynomial regression models. Dataset used has two features.\n",
    "utils.train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=6, baseline=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lambdas to plot\n",
    "reg_params = [10, 5, 2, 1, 0.5, 0.2, 0.1]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "utils.train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The resulting plot shows an initial $\\lambda$ of `10` and as you can see, the training error is worse than the baseline at that point. This implies that it is placing a huge penalty on the `w` parameters and this prevents the model from learning more complex patterns in your data. As you decrease $\\lambda$, the model loosens this restriction and the training error is able to approach the baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Fixing High Variance\n",
    "\n",
    "You will now look at some things to try when your model has overfit the training set. The main objective is to have a model that generalizes well to new examples so you want to minimize the cross validation error.\n",
    "\n",
    "### Try increasing the regularization parameter\n",
    "\n",
    "In contrast to the last exercise above, setting a very small value of the regularization parameter will keep the model low bias but might not do much to improve the variance. As shown below, you can improve your cross validation error by increasing the value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lambdas to plot\n",
    "reg_params = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "utils.train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller set of features\n",
    "\n",
    "# Prepare dataset with randomID feature\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data2.csv')\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training set with 2 features:\\n {x_train[:5]}\\n\")\n",
    "\n",
    "# Prepare dataset with randomID feature\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data3.csv')\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training set with 3 features (1st column is a random ID):\\n {x_train[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define properties of the 2 datasets\n",
    "file1 = {'filename':'data/c2w3_lab2_data3.csv', 'label': '3 features', 'linestyle': 'dotted'}\n",
    "file2 = {'filename':'data/c2w3_lab2_data2.csv', 'label': '2 features', 'linestyle': 'solid'}\n",
    "files = [file1, file2]\n",
    "\n",
    "# Train and plot for each dataset\n",
    "utils.train_plot_diff_datasets(model, files, max_degree=4, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Get more training examples\n",
    "\n",
    "Lastly, you can try to minimize the cross validation error by getting more examples. In the cell below, you will train a 4th degree polynomial model then plot the *learning curve* of your model to see how the errors behave when you get more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data4.csv')\n",
    "print(f\"the shape of the entire training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the entire training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the entire cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the entire cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "\n",
    "# Instantiate the model class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define the degree of polynomial and train the model using subsets of the dataset.\n",
    "utils.train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
